{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ef2eabe",
      "metadata": {
        "id": "4ef2eabe"
      },
      "source": [
        "# Lab 4: Design and Query OLTP and OLAP Systems (PostgreSQL, Google Colab Version)\n",
        "\n",
        "In this Colab version, we:\n",
        "\n",
        "- Install and start PostgreSQL directly inside the Colab runtime\n",
        "- Configure the `postgres` user so Python can connect with a password\n",
        "- Use Python (`psycopg2`) to send SQL commands to PostgreSQL\n",
        "- Keep **all major steps** of the lab:\n",
        "  1. Set up PostgreSQL and connect to the database  \n",
        "  2. Implement an **OLTP** (normalized) schema  \n",
        "  3. Implement an **OLAP** (star) schema  \n",
        "  4. Populate both schemas with synthetic data  \n",
        "  5. Compare **OLTP vs. OLAP** query performance using `EXPLAIN ANALYZE`\n",
        "\n",
        "> ðŸ’¡ **Goal:** Experience how OLTP (transactional) and OLAP (analytical) designs differ in structure and performance, using the same underlying data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdca5963",
      "metadata": {
        "id": "fdca5963"
      },
      "source": [
        "## Task 1 â€“ Setting up PostgreSQL and Connecting to the Database\n",
        "\n",
        "### Original Lab Instructions (for reference)\n",
        "\n",
        "\n",
        "In this **Google Colab**, we:\n",
        "\n",
        "1. Install PostgreSQL and its tools directly in the Colab environment  \n",
        "2. Start the PostgreSQL service  \n",
        "3. Set a password for the `postgres` user  \n",
        "4. Connect from Python using `psycopg2`\n",
        "\n",
        "Run the following cell **once** at the top of your session to set up PostgreSQL.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2ca132e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ca132e4",
        "outputId": "1f8c784d-3a34-49d9-c20f-7a567c8c3233"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.83)] [Connected to cloud.r-p\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [Connected to security.ubuntu.com (185.125.\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [Waiting for headers] [Connected to cloud.r\r                                                                               \rHit:4 https://cli.github.com/packages stable InRelease\n",
            "\r0% [2 InRelease 15.6 kB/128 kB 12%] [Waiting for headers] [Waiting for headers]\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [2 InRelease 70.6 kB/128 kB 55%] [Waiting for headers] [5 InRelease 3,632 B/\r0% [2 InRelease 70.6 kB/128 kB 55%] [Waiting for headers] [Waiting for headers]\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rGet:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [6 InRelease 6,555 B/6,555 B 100\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rGet:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,836 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Fetched 37.5 MB in 4s (9,980 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common ssl-cert\n",
            "  sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common\n",
            "  postgresql-contrib ssl-cert sysstat\n",
            "0 upgraded, 15 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 42.4 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjson-xs-perl amd64 4.040-0ubuntu0.22.04.1 [87.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm14 amd64 1:14.0.0-1ubuntu1.1 [24.0 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.19-0ubuntu0.22.04.1 [1,249 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.19-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 42.4 MB in 1s (60.3 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libllvm14:amd64.\n",
            "Preparing to unpack .../06-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../07-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../08-postgresql-client-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../09-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../10-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../11-postgresql-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../12-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../13-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../14-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer â†’ /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer â†’ /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer â†’ /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service â†’ /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service â†’ /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Task 1 â€“ Install and start PostgreSQL, and install Python dependencies\n",
        "# This may take a few minutes the first time it runs.\n",
        "\n",
        "!apt-get -y update\n",
        "!apt-get -y install postgresql postgresql-contrib\n",
        "\n",
        "# Start the PostgreSQL service\n",
        "!service postgresql start\n",
        "\n",
        "# Set a password for the 'postgres' superuser so we can connect via TCP\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "\n",
        "# Install psycopg2 so Python can talk to PostgreSQL\n",
        "!pip install -q psycopg2-binary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01fa415d",
      "metadata": {
        "id": "01fa415d"
      },
      "source": [
        "### Connect to PostgreSQL from Python\n",
        "\n",
        "Below we:\n",
        "\n",
        "1. Connect to the default `postgres` database as the `postgres` superuser  \n",
        "2. Drop and recreate a clean `query_demo` database  \n",
        "3. Connect to `query_demo`  \n",
        "4. Define a helper function `run_sql` to send SQL commands and optionally print results\n",
        "\n",
        "> ðŸ” If you re-run this notebook from the top, the database will be recreated fresh each time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "29a9e76f",
      "metadata": {
        "id": "29a9e76f"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the default 'postgres' database to manage databases.\n",
        "# We connect over TCP to 127.0.0.1 using the password we just set.\n",
        "admin_conn = psycopg2.connect(\n",
        "    dbname=\"postgres\",\n",
        "    user=\"postgres\",\n",
        "    password=\"postgres\",\n",
        "    host=\"127.0.0.1\",\n",
        "    port=5432\n",
        ")\n",
        "admin_conn.autocommit = True\n",
        "admin_cur = admin_conn.cursor()\n",
        "\n",
        "# Drop and recreate the lab database to ensure a clean environment\n",
        "admin_cur.execute(\"DROP DATABASE IF EXISTS query_demo;\")\n",
        "admin_cur.execute(\"CREATE DATABASE query_demo;\")\n",
        "\n",
        "admin_cur.close()\n",
        "admin_conn.close()\n",
        "\n",
        "# Connect to the lab database\n",
        "conn = psycopg2.connect(\n",
        "    dbname=\"query_demo\",\n",
        "    user=\"postgres\",\n",
        "    password=\"postgres\",\n",
        "    host=\"127.0.0.1\",\n",
        "    port=5432\n",
        ")\n",
        "conn.autocommit = True\n",
        "cur = conn.cursor()\n",
        "\n",
        "def run_sql(sql, fetch=False, max_rows=20):\n",
        "    \"\"\"Helper to run SQL against PostgreSQL and optionally print results.\n",
        "\n",
        "    Args:\n",
        "        sql (str): SQL command(s) to execute.\n",
        "        fetch (bool): If True, fetch and print results.\n",
        "        max_rows (int): Max number of rows to display when fetching.\n",
        "    \"\"\"\n",
        "    print(\"Running SQL:\\n\" + \"-\" * 80)\n",
        "    print(sql.strip())\n",
        "    print(\"-\" * 80)\n",
        "    cur.execute(sql)\n",
        "\n",
        "    if fetch:\n",
        "        rows = cur.fetchall()\n",
        "        print(f\"Returned {len(rows)} rows (showing up to {max_rows}):\")\n",
        "        for row in rows[:max_rows]:\n",
        "            print(row)\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"Rowcount: {cur.rowcount}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9ab3618",
      "metadata": {
        "id": "c9ab3618"
      },
      "source": [
        "## Task 2 â€“ Implementing an OLTP System (Normalized Schema)\n",
        "\n",
        "In an **OLTP** (Online Transaction Processing) system, the schema is typically **normalized** to:\n",
        "\n",
        "- Reduce data redundancy  \n",
        "- Maintain data integrity (through foreign keys)  \n",
        "- Support many small, frequent **INSERT/UPDATE/DELETE** operations  \n",
        "\n",
        "In this lab, the normalized OLTP schema consists of:\n",
        "\n",
        "- **`customers`** â€“ one row per customer  \n",
        "- **`products`** â€“ one row per product  \n",
        "- **`orders`** â€“ one row per order (header)  \n",
        "- **`order_items`** â€“ one row per line item on an order  \n",
        "\n",
        "We will now create these tables exactly as in the original lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "db353d65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db353d65",
        "outputId": "18e68ed5-f052-444c-9249-2eb03bcf607b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "DROP TABLE IF EXISTS order_items CASCADE;\n",
            "DROP TABLE IF EXISTS orders CASCADE;\n",
            "DROP TABLE IF EXISTS products CASCADE;\n",
            "DROP TABLE IF EXISTS customers CASCADE;\n",
            "\n",
            "CREATE TABLE customers (\n",
            "    customer_id SERIAL PRIMARY KEY,\n",
            "    name VARCHAR(100),\n",
            "    email VARCHAR(100) UNIQUE\n",
            ");\n",
            "\n",
            "CREATE TABLE products (\n",
            "    product_id SERIAL PRIMARY KEY,\n",
            "    name VARCHAR(100),\n",
            "    price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE orders (\n",
            "    order_id SERIAL PRIMARY KEY,\n",
            "    customer_id INT REFERENCES customers(customer_id),\n",
            "    order_date TIMESTAMP DEFAULT NOW()\n",
            ");\n",
            "\n",
            "CREATE TABLE order_items (\n",
            "    order_item_id SERIAL PRIMARY KEY,\n",
            "    order_id INT REFERENCES orders(order_id) ON DELETE CASCADE,\n",
            "    product_id INT REFERENCES products(product_id),\n",
            "    quantity INT,\n",
            "    price DECIMAL(10,2)\n",
            ");\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: -1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 2 â€“ Create OLTP (normalized) tables\n",
        "create_oltp_sql = \"\"\"\n",
        "DROP TABLE IF EXISTS order_items CASCADE;\n",
        "DROP TABLE IF EXISTS orders CASCADE;\n",
        "DROP TABLE IF EXISTS products CASCADE;\n",
        "DROP TABLE IF EXISTS customers CASCADE;\n",
        "\n",
        "CREATE TABLE customers (\n",
        "    customer_id SERIAL PRIMARY KEY,\n",
        "    name VARCHAR(100),\n",
        "    email VARCHAR(100) UNIQUE\n",
        ");\n",
        "\n",
        "CREATE TABLE products (\n",
        "    product_id SERIAL PRIMARY KEY,\n",
        "    name VARCHAR(100),\n",
        "    price DECIMAL(10,2)\n",
        ");\n",
        "\n",
        "CREATE TABLE orders (\n",
        "    order_id SERIAL PRIMARY KEY,\n",
        "    customer_id INT REFERENCES customers(customer_id),\n",
        "    order_date TIMESTAMP DEFAULT NOW()\n",
        ");\n",
        "\n",
        "CREATE TABLE order_items (\n",
        "    order_item_id SERIAL PRIMARY KEY,\n",
        "    order_id INT REFERENCES orders(order_id) ON DELETE CASCADE,\n",
        "    product_id INT REFERENCES products(product_id),\n",
        "    quantity INT,\n",
        "    price DECIMAL(10,2)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "run_sql(create_oltp_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11b2b13d",
      "metadata": {
        "id": "11b2b13d"
      },
      "source": [
        "## Task 3 â€“ Implementing an OLAP System (Star Schema)\n",
        "\n",
        "An **OLAP** (Online Analytical Processing) system is optimized for **analytics and reporting**, not for frequent small transactions.\n",
        "\n",
        "A common OLAP design is the **star schema**, consisting of:\n",
        "\n",
        "- A central **fact table** with metrics (e.g. `fact_sales`)  \n",
        "- Several surrounding **dimension tables** with descriptive attributes (e.g. `dim_customers`, `dim_products`)  \n",
        "\n",
        "In this lab, the OLAP schema includes:\n",
        "\n",
        "- **`dim_customers`** â€“ dimension table with customer information  \n",
        "- **`dim_products`** â€“ dimension table with product information  \n",
        "- **`fact_sales`** â€“ fact table with sales transactions (quantities and prices)  \n",
        "\n",
        "We will now create these star-schema tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7394485d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7394485d",
        "outputId": "47f28963-93e2-469a-c40e-244fb454161a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "DROP TABLE IF EXISTS fact_sales CASCADE;\n",
            "DROP TABLE IF EXISTS dim_products CASCADE;\n",
            "DROP TABLE IF EXISTS dim_customers CASCADE;\n",
            "\n",
            "CREATE TABLE dim_customers (\n",
            "    customer_key SERIAL PRIMARY KEY,\n",
            "    customer_id INT UNIQUE,\n",
            "    name VARCHAR(100),\n",
            "    email VARCHAR(100)\n",
            ");\n",
            "\n",
            "CREATE TABLE dim_products (\n",
            "    product_key SERIAL PRIMARY KEY,\n",
            "    product_id INT UNIQUE,\n",
            "    name VARCHAR(100),\n",
            "    price DECIMAL(10,2)\n",
            ");\n",
            "\n",
            "CREATE TABLE fact_sales (\n",
            "    sale_key SERIAL PRIMARY KEY,\n",
            "    customer_key INT REFERENCES dim_customers(customer_key),\n",
            "    product_key INT REFERENCES dim_products(product_key),\n",
            "    sale_date TIMESTAMP DEFAULT NOW(),\n",
            "    quantity INT,\n",
            "    total_price DECIMAL(10,2)\n",
            ");\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: -1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 3 â€“ Create OLAP (star schema) tables\n",
        "create_olap_sql = \"\"\"\n",
        "DROP TABLE IF EXISTS fact_sales CASCADE;\n",
        "DROP TABLE IF EXISTS dim_products CASCADE;\n",
        "DROP TABLE IF EXISTS dim_customers CASCADE;\n",
        "\n",
        "CREATE TABLE dim_customers (\n",
        "    customer_key SERIAL PRIMARY KEY,\n",
        "    customer_id INT UNIQUE,\n",
        "    name VARCHAR(100),\n",
        "    email VARCHAR(100)\n",
        ");\n",
        "\n",
        "CREATE TABLE dim_products (\n",
        "    product_key SERIAL PRIMARY KEY,\n",
        "    product_id INT UNIQUE,\n",
        "    name VARCHAR(100),\n",
        "    price DECIMAL(10,2)\n",
        ");\n",
        "\n",
        "CREATE TABLE fact_sales (\n",
        "    sale_key SERIAL PRIMARY KEY,\n",
        "    customer_key INT REFERENCES dim_customers(customer_key),\n",
        "    product_key INT REFERENCES dim_products(product_key),\n",
        "    sale_date TIMESTAMP DEFAULT NOW(),\n",
        "    quantity INT,\n",
        "    total_price DECIMAL(10,2)\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "run_sql(create_olap_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c4d232",
      "metadata": {
        "id": "e6c4d232"
      },
      "source": [
        "## Task 4 â€“ Populating OLTP and OLAP Databases\n",
        "\n",
        "To compare OLTP and OLAP performance meaningfully, we need **lots of data**.\n",
        "\n",
        "We will populate the database as follows:\n",
        "\n",
        "1. Insert **10 customers** with synthetic names and emails  \n",
        "2. Insert **5 products** with fixed names and prices  \n",
        "3. Insert **100,000 orders** with:\n",
        "   - Random customers  \n",
        "   - Random order dates within the last year  \n",
        "4. Insert **100,000 order items** with:\n",
        "   - Random product  \n",
        "   - Random quantity  \n",
        "   - Random price  \n",
        "5. Populate the OLAP tables by:\n",
        "   - Copying customers into `dim_customers`  \n",
        "   - Copying products into `dim_products`  \n",
        "   - Creating `fact_sales` rows from the OLTP `orders` and `order_items` tables  \n",
        "\n",
        "The synthetic data is generated **inside PostgreSQL** using `generate_series` and `random()`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9df30a6c",
      "metadata": {
        "id": "9df30a6c"
      },
      "source": [
        "### 4.1 Insert Sample Customers\n",
        "\n",
        "We create 10 synthetic customers with names like `Customer_1`, `Customer_2`, etc., and matching email addresses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9d1d1be4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d1d1be4",
        "outputId": "60a1f05d-d317-4c12-a362-7f689b2189b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "INSERT INTO customers (name, email) \n",
            "SELECT 'Customer_' || i, 'customer' || i || '@example.com'\n",
            "FROM generate_series(1, 10) AS i;\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 10\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "insert_customers_sql = \"\"\"\n",
        "INSERT INTO customers (name, email)\n",
        "SELECT 'Customer_' || i, 'customer' || i || '@example.com'\n",
        "FROM generate_series(1, 10) AS i;\n",
        "\"\"\"\n",
        "\n",
        "run_sql(insert_customers_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8be7826",
      "metadata": {
        "id": "f8be7826"
      },
      "source": [
        "### 4.2 Insert Sample Products\n",
        "\n",
        "We insert a small product catalog of 5 products with fixed prices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bde124a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bde124a0",
        "outputId": "5d7e5967-7362-4527-ce28-ca298c20261f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "INSERT INTO products (name, price) \n",
            "VALUES ('Laptop', 1200.00),\n",
            "       ('Mouse', 25.00),\n",
            "       ('Keyboard', 50.00),\n",
            "       ('Monitor', 300.00),\n",
            "       ('Phone', 800.00);\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 5\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "insert_products_sql = \"\"\"\n",
        "INSERT INTO products (name, price)\n",
        "VALUES ('Laptop', 1200.00),\n",
        "       ('Mouse', 25.00),\n",
        "       ('Keyboard', 50.00),\n",
        "       ('Monitor', 300.00),\n",
        "       ('Phone', 800.00);\n",
        "\"\"\"\n",
        "\n",
        "run_sql(insert_products_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1736eae",
      "metadata": {
        "id": "e1736eae"
      },
      "source": [
        "### 4.3 Insert 100,000 Orders\n",
        "\n",
        "Each order:\n",
        "\n",
        "- Is assigned to a **random customer** from 1 to 10  \n",
        "- Gets an `order_date` randomly distributed over the last 365 days  \n",
        "\n",
        "> âš ï¸ This step may take a little time, since it creates 100,000 rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f7101e8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7101e8e",
        "outputId": "09afcda9-9d02-456e-e53b-8d92e01aba01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "INSERT INTO orders (customer_id, order_date)\n",
            "SELECT (random() * 9 + 1)::INT, NOW() - (random() * INTERVAL '365 days')\n",
            "FROM generate_series(1, 100000);\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 100000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "insert_orders_sql = \"\"\"\n",
        "INSERT INTO orders (customer_id, order_date)\n",
        "SELECT (random() * 9 + 1)::INT, NOW() - (random() * INTERVAL '365 days')\n",
        "FROM generate_series(1, 100000);\n",
        "\"\"\"\n",
        "\n",
        "run_sql(insert_orders_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8906f05c",
      "metadata": {
        "id": "8906f05c"
      },
      "source": [
        "### 4.4 Insert 100,000 Order Items\n",
        "\n",
        "Each order item row:\n",
        "\n",
        "- Is linked to an existing `orders.order_id`  \n",
        "- Uses a random `product_id` from 1 to 5  \n",
        "- Has a random quantity from 1 to 4  \n",
        "- Has a random price between 20 and 320  \n",
        "\n",
        "> Again, this may take a little time because of the volume of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9f894463",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f894463",
        "outputId": "792bbb44-5b8a-4eb4-afb6-567c30945f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "INSERT INTO order_items (order_id, product_id, quantity, price)\n",
            "SELECT o.order_id,\n",
            "       (random() * 4 + 1)::INT,\n",
            "       (random() * 3 + 1)::INT,\n",
            "       (random() * 300 + 20)::NUMERIC\n",
            "FROM orders o\n",
            "ORDER BY random()\n",
            "LIMIT 100000;\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 100000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "insert_order_items_sql = \"\"\"\n",
        "INSERT INTO order_items (order_id, product_id, quantity, price)\n",
        "SELECT o.order_id,\n",
        "       (random() * 4 + 1)::INT,\n",
        "       (random() * 3 + 1)::INT,\n",
        "       (random() * 300 + 20)::NUMERIC\n",
        "FROM orders o\n",
        "ORDER BY random()\n",
        "LIMIT 100000;\n",
        "\"\"\"\n",
        "\n",
        "run_sql(insert_order_items_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf17e08",
      "metadata": {
        "id": "abf17e08"
      },
      "source": [
        "### 4.5 Populate OLAP (Star Schema) Tables\n",
        "\n",
        "Now we:\n",
        "\n",
        "1. Copy customers from `customers` â†’ `dim_customers`  \n",
        "2. Copy products from `products` â†’ `dim_products`  \n",
        "3. Create fact rows in `fact_sales` by joining `orders`, `order_items`, `dim_customers`, and `dim_products`  \n",
        "\n",
        "This simulates a typical **ETL (Extractâ€“Transformâ€“Load)** process from OLTP into OLAP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "98a51a27",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98a51a27",
        "outputId": "c33e2d9f-5830-44ed-de22-28e08a632d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "-- 1. Populate dim_customers\n",
            "INSERT INTO dim_customers (customer_id, name, email)\n",
            "SELECT customer_id, name, email\n",
            "FROM customers;\n",
            "\n",
            "-- 2. Populate dim_products\n",
            "INSERT INTO dim_products (product_id, name, price)\n",
            "SELECT product_id, name, price\n",
            "FROM products;\n",
            "\n",
            "-- 3. Populate fact_sales\n",
            "INSERT INTO fact_sales (customer_key, product_key, sale_date, quantity, total_price)\n",
            "SELECT c.customer_key,\n",
            "       p.product_key,\n",
            "       o.order_date,\n",
            "       oi.quantity,\n",
            "       (oi.price * oi.quantity)\n",
            "FROM order_items oi\n",
            "JOIN orders o ON oi.order_id = o.order_id\n",
            "JOIN dim_customers c ON o.customer_id = c.customer_id\n",
            "JOIN dim_products p ON oi.product_id = p.product_id;\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 100000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "populate_olap_sql = \"\"\"\n",
        "-- 1. Populate dim_customers\n",
        "INSERT INTO dim_customers (customer_id, name, email)\n",
        "SELECT customer_id, name, email\n",
        "FROM customers;\n",
        "\n",
        "-- 2. Populate dim_products\n",
        "INSERT INTO dim_products (product_id, name, price)\n",
        "SELECT product_id, name, price\n",
        "FROM products;\n",
        "\n",
        "-- 3. Populate fact_sales\n",
        "INSERT INTO fact_sales (customer_key, product_key, sale_date, quantity, total_price)\n",
        "SELECT c.customer_key,\n",
        "       p.product_key,\n",
        "       o.order_date,\n",
        "       oi.quantity,\n",
        "       (oi.price * oi.quantity)\n",
        "FROM order_items oi\n",
        "JOIN orders o ON oi.order_id = o.order_id\n",
        "JOIN dim_customers c ON o.customer_id = c.customer_id\n",
        "JOIN dim_products p ON oi.product_id = p.product_id;\n",
        "\"\"\"\n",
        "\n",
        "run_sql(populate_olap_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61ea8ca1",
      "metadata": {
        "id": "61ea8ca1"
      },
      "source": [
        "### 4.6 Sanity Check â€“ Row Counts\n",
        "\n",
        "Letâ€™s quickly confirm that the tables contain data as expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c0cdcd19",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0cdcd19",
        "outputId": "eee5413a-5def-4b9d-c86f-1ebf7cec5c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM customers;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(10,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM products;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(5,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM orders;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(100000,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM order_items;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(100000,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM dim_customers;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(10,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM dim_products;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(5,)\n",
            "\n",
            "\n",
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT COUNT(*) FROM fact_sales;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 1 rows (showing up to 5):\n",
            "(100000,)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for table in [\"customers\", \"products\", \"orders\", \"order_items\",\n",
        "               \"dim_customers\", \"dim_products\", \"fact_sales\"]:\n",
        "    run_sql(f\"SELECT COUNT(*) FROM {table};\", fetch=True, max_rows=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ce810d0",
      "metadata": {
        "id": "7ce810d0"
      },
      "source": [
        "## Task 5 â€“ Comparing OLTP vs. OLAP Query Performance\n",
        "\n",
        "Now that we have large datasets in both schemas, we can compare query performance.\n",
        "\n",
        "Weâ€™ll use `EXPLAIN ANALYZE` to measure the execution plan and timing for:\n",
        "\n",
        "1. An aggregation query on the **normalized OLTP schema** (joins across `orders`, `customers`, `order_items`, `products`)  \n",
        "2. An aggregation query on the **OLAP star schema** (joins across `fact_sales`, `dim_customers`, `dim_products`)  \n",
        "\n",
        "In both cases, we compute:\n",
        "\n",
        "- Total quantity per (customer, product)  \n",
        "- Total revenue per (customer, product)  \n",
        "\n",
        "> ðŸ’¡ Watch how the execution time and join complexity differ between the two designs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e434ffb",
      "metadata": {
        "id": "4e434ffb"
      },
      "source": [
        "### 5.1 OLTP Query Performance (Normalized Schema)\n",
        "\n",
        "This query uses the **normalized** tables directly:\n",
        "\n",
        "```sql\n",
        "SELECT c.name, p.name, SUM(oi.quantity) AS total_quantity,\n",
        "       SUM(oi.price * oi.quantity) AS total_revenue\n",
        "FROM orders o\n",
        "JOIN customers c ON o.customer_id = c.customer_id\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "JOIN products p ON oi.product_id = p.product_id\n",
        "GROUP BY c.name, p.name;\n",
        "```\n",
        "\n",
        "Weâ€™ll wrap it with `EXPLAIN ANALYZE` to see the execution plan and timing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bf2011c9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2011c9",
        "outputId": "6f287d60-7fb8-4fea-b380-950510ee592b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "EXPLAIN ANALYZE\n",
            "SELECT c.name, p.name, SUM(oi.quantity) AS total_quantity, \n",
            "       SUM(oi.price * oi.quantity) AS total_revenue\n",
            "FROM orders o\n",
            "JOIN customers c ON o.customer_id = c.customer_id\n",
            "JOIN order_items oi ON o.order_id = oi.order_id\n",
            "JOIN products p ON oi.product_id = p.product_id\n",
            "GROUP BY c.name, p.name;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 23 rows (showing up to 50):\n",
            "('GroupAggregate  (cost=35682.15..37857.15 rows=34000 width=476) (actual time=236.459..288.415 rows=50 loops=1)',)\n",
            "('  Group Key: c.name, p.name',)\n",
            "('  ->  Sort  (cost=35682.15..35932.15 rows=100000 width=446) (actual time=235.808..256.730 rows=100000 loops=1)',)\n",
            "('        Sort Key: c.name, p.name',)\n",
            "('        Sort Method: external merge  Disk: 4032kB',)\n",
            "('        ->  Hash Join  (cost=3212.57..7210.33 rows=100000 width=446) (actual time=27.986..141.461 rows=100000 loops=1)',)\n",
            "('              Hash Cond: (oi.product_id = p.product_id)',)\n",
            "('              ->  Hash Join  (cost=3195.82..6927.38 rows=100000 width=232) (actual time=27.956..124.808 rows=100000 loops=1)',)\n",
            "('                    Hash Cond: (o.customer_id = c.customer_id)',)\n",
            "('                    ->  Hash Join  (cost=3182.00..6644.51 rows=100000 width=18) (actual time=27.931..110.039 rows=100000 loops=1)',)\n",
            "('                          Hash Cond: (oi.order_id = o.order_id)',)\n",
            "('                          ->  Seq Scan on order_items oi  (cost=0.00..1637.00 rows=100000 width=18) (actual time=0.006..10.844 rows=100000 loops=1)',)\n",
            "('                          ->  Hash  (cost=1541.00..1541.00 rows=100000 width=8) (actual time=27.686..27.688 rows=100000 loops=1)',)\n",
            "('                                Buckets: 131072  Batches: 2  Memory Usage: 2976kB',)\n",
            "('                                ->  Seq Scan on orders o  (cost=0.00..1541.00 rows=100000 width=8) (actual time=0.005..9.227 rows=100000 loops=1)',)\n",
            "('                    ->  Hash  (cost=11.70..11.70 rows=170 width=222) (actual time=0.019..0.020 rows=10 loops=1)',)\n",
            "('                          Buckets: 1024  Batches: 1  Memory Usage: 9kB',)\n",
            "('                          ->  Seq Scan on customers c  (cost=0.00..11.70 rows=170 width=222) (actual time=0.004..0.006 rows=10 loops=1)',)\n",
            "('              ->  Hash  (cost=13.00..13.00 rows=300 width=222) (actual time=0.017..0.018 rows=5 loops=1)',)\n",
            "('                    Buckets: 1024  Batches: 1  Memory Usage: 9kB',)\n",
            "('                    ->  Seq Scan on products p  (cost=0.00..13.00 rows=300 width=222) (actual time=0.010..0.011 rows=5 loops=1)',)\n",
            "('Planning Time: 0.632 ms',)\n",
            "('Execution Time: 289.292 ms',)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "oltp_explain_sql = \"\"\"\n",
        "EXPLAIN ANALYZE\n",
        "SELECT c.name, p.name, SUM(oi.quantity) AS total_quantity,\n",
        "       SUM(oi.price * oi.quantity) AS total_revenue\n",
        "FROM orders o\n",
        "JOIN customers c ON o.customer_id = c.customer_id\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "JOIN products p ON oi.product_id = p.product_id\n",
        "GROUP BY c.name, p.name;\n",
        "\"\"\"\n",
        "\n",
        "run_sql(oltp_explain_sql, fetch=True, max_rows=50)  # show first 50 lines of the plan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8fb8fcf",
      "metadata": {
        "id": "f8fb8fcf"
      },
      "source": [
        "### 5.2 OLAP Query Performance (Star Schema)\n",
        "\n",
        "Now we run the equivalent aggregation query against the **OLAP star schema**:\n",
        "\n",
        "```sql\n",
        "SELECT c.name, p.name, SUM(f.quantity) AS total_quantity,\n",
        "       SUM(f.total_price) AS total_revenue\n",
        "FROM fact_sales f\n",
        "JOIN dim_customers c ON f.customer_key = c.customer_key\n",
        "JOIN dim_products p ON f.product_key = p.product_key\n",
        "GROUP BY c.name, p.name;\n",
        "```\n",
        "\n",
        "Again, we wrap it in `EXPLAIN ANALYZE` to inspect the plan and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f74b4568",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f74b4568",
        "outputId": "999fb5d2-6378-4762-e78b-43194fa2baa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "EXPLAIN ANALYZE\n",
            "SELECT c.name, p.name, SUM(f.quantity) AS total_quantity, \n",
            "       SUM(f.total_price) AS total_revenue\n",
            "FROM fact_sales f\n",
            "JOIN dim_customers c ON f.customer_key = c.customer_key\n",
            "JOIN dim_products p ON f.product_key = p.product_key\n",
            "GROUP BY c.name, p.name;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 18 rows (showing up to 50):\n",
            "('GroupAggregate  (cost=30871.64..32546.64 rows=34000 width=476) (actual time=149.989..184.415 rows=50 loops=1)',)\n",
            "('  Group Key: c.name, p.name',)\n",
            "('  ->  Sort  (cost=30871.64..31121.64 rows=100000 width=446) (actual time=149.290..168.086 rows=100000 loops=1)',)\n",
            "('        Sort Key: c.name, p.name',)\n",
            "('        Sort Method: external merge  Disk: 4032kB',)\n",
            "('        ->  Hash Join  (cost=30.57..2399.82 rows=100000 width=446) (actual time=0.040..40.524 rows=100000 loops=1)',)\n",
            "('              Hash Cond: (f.product_key = p.product_key)',)\n",
            "('              ->  Hash Join  (cost=13.82..2116.87 rows=100000 width=232) (actual time=0.018..25.109 rows=100000 loops=1)',)\n",
            "('                    Hash Cond: (f.customer_key = c.customer_key)',)\n",
            "('                    ->  Seq Scan on fact_sales f  (cost=0.00..1834.00 rows=100000 width=18) (actual time=0.004..6.501 rows=100000 loops=1)',)\n",
            "('                    ->  Hash  (cost=11.70..11.70 rows=170 width=222) (actual time=0.008..0.009 rows=10 loops=1)',)\n",
            "('                          Buckets: 1024  Batches: 1  Memory Usage: 9kB',)\n",
            "('                          ->  Seq Scan on dim_customers c  (cost=0.00..11.70 rows=170 width=222) (actual time=0.004..0.006 rows=10 loops=1)',)\n",
            "('              ->  Hash  (cost=13.00..13.00 rows=300 width=222) (actual time=0.017..0.017 rows=5 loops=1)',)\n",
            "('                    Buckets: 1024  Batches: 1  Memory Usage: 9kB',)\n",
            "('                    ->  Seq Scan on dim_products p  (cost=0.00..13.00 rows=300 width=222) (actual time=0.011..0.012 rows=5 loops=1)',)\n",
            "('Planning Time: 0.327 ms',)\n",
            "('Execution Time: 184.989 ms',)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "olap_explain_sql = \"\"\"\n",
        "EXPLAIN ANALYZE\n",
        "SELECT c.name, p.name, SUM(f.quantity) AS total_quantity,\n",
        "       SUM(f.total_price) AS total_revenue\n",
        "FROM fact_sales f\n",
        "JOIN dim_customers c ON f.customer_key = c.customer_key\n",
        "JOIN dim_products p ON f.product_key = p.product_key\n",
        "GROUP BY c.name, p.name;\n",
        "\"\"\"\n",
        "\n",
        "run_sql(olap_explain_sql, fetch=True, max_rows=50)  # show first 50 lines of the plan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea83346",
      "metadata": {
        "id": "7ea83346"
      },
      "source": [
        "## Performance Summary and Reflection\n",
        "\n",
        "In general, you should observe something like:\n",
        "\n",
        "| Aspect                 | OLTP (Normalized Schema)                     | OLAP (Star Schema)                          |\n",
        "|------------------------|----------------------------------------------|---------------------------------------------|\n",
        "| Query speed            | Slower for large aggregations                | Faster for analytics                        |\n",
        "| JOINs required         | Many (e.g., 4+ tables)                       | Fewer (2â€“3 tables)                          |\n",
        "| Best for               | Transactions (INSERTs, UPDATEs, small reads) | Reporting, BI, aggregations                 |\n",
        "| Storage                | More efficient (less redundancy)             | Larger (pre-aggregated / duplicated data)   |\n",
        "\n",
        "### Lab Review Questions\n",
        "\n",
        "1. **Which system (OLTP or OLAP) performed faster for aggregations in your run? Why do you think that is?**  \n",
        "2. **How would indexing affect OLTP query performance?**  \n",
        "   - A. Improve performance for reads  \n",
        "   - B. Slow down inserts  \n",
        "   - C. No impact  \n",
        "   - D. Both A and B  \n",
        "\n",
        "Think about the trade-offs between **write performance**, **read performance**, and **storage** when choosing between normalized and star schemas in real-world systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e0391a2",
      "metadata": {
        "id": "5e0391a2"
      },
      "source": [
        "### (Optional) Close the Database Connection\n",
        "\n",
        "When youâ€™re completely done with the lab, you can close the PostgreSQL connection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e314bbaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e314bbaa",
        "outputId": "f7e0e691-88ee-4af5-b88c-1cae56d12dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PostgreSQL connection closed.\n"
          ]
        }
      ],
      "source": [
        "# Optional: close the connection when finished\n",
        "cur.close()\n",
        "conn.close()\n",
        "print(\"PostgreSQL connection closed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}