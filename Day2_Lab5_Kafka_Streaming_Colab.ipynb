{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3093b9ca",
      "metadata": {
        "id": "3093b9ca"
      },
      "source": [
        "# Lab 5: Explore Kafka Streaming (Google Colab Version)\n",
        "\n",
        "In this Colab version we:\n",
        "\n",
        "- Download and run **Apache Kafka** directly inside the Colab environment  \n",
        "- Use Kafkaâ€™s command-line tools (`kafka-topics.sh`, `kafka-console-producer.sh`, `kafka-console-consumer.sh`)  \n",
        "- Script the messages instead of typing everything manually, so the lab is fully reproducible  \n",
        "\n",
        "> ðŸ’¡ **Goal:** Experience how Apache Kafka handles real-time data streaming, topics, producers, consumers, and fault tolerance â€” all from a single Colab notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d6ede3b",
      "metadata": {
        "id": "4d6ede3b"
      },
      "source": [
        "## Task 1 â€“ Setting up a Kafka Cluster\n",
        "\n",
        "\n",
        "In **Colab** we:\n",
        "\n",
        "1. Install Java (required for Kafka)  \n",
        "2. Download and extract a Kafka binary distribution  \n",
        "3. Start **Zookeeper** and **Kafka** as background processes inside the Colab VM  \n",
        "\n",
        "Run the following cell **once** at the start of the lab to set up Kafka.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce2e3721",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2e3721",
        "outputId": "b857909c-e743-4c64-c970-1ed5ab4bad64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:20 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,836 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Fetched 37.5 MB in 9s (4,223 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic fonts-ipafont-mincho\n",
            "  fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-11-jre-headless\n",
            "0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 42.6 MB of archives.\n",
            "After this operation, 176 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 openjdk-11-jre-headless amd64 11.0.29+7-1ubuntu1~22.04 [42.6 MB]\n",
            "Fetched 42.6 MB in 4s (10.5 MB/s)\n",
            "Selecting previously unselected package openjdk-11-jre-headless:amd64.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-11-jre-headless_11.0.29+7-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "Setting up openjdk-11-jre-headless:amd64 (11.0.29+7-1ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-11-openjdk-amd64/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode\n",
            "tar (child): kafka_2.13-3.7.0.tgz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n",
            "mv: cannot stat 'kafka_2.13-3.7.0': No such file or directory\n",
            "/bin/bash: line 1: kafka/bin/zookeeper-server-start.sh: No such file or directory\n",
            "Starting Zookeeper...\n",
            "/bin/bash: line 1: kafka/bin/kafka-server-start.sh: No such file or directory\n",
            "Starting Kafka broker...\n",
            "Kafka cluster should now be running on localhost:9092.\n"
          ]
        }
      ],
      "source": [
        "# Task 1 â€“ Install Java, download Kafka, and start Zookeeper + Kafka\n",
        "# This may take a few minutes the first time it runs.\n",
        "\n",
        "import os, time\n",
        "\n",
        "# Install Java (needed by Kafka)\n",
        "!apt-get -y update\n",
        "!apt-get -y install openjdk-11-jre-headless\n",
        "\n",
        "# Download Kafka (you can change the version if needed)\n",
        "if not os.path.exists(\"kafka\"):\n",
        "    !wget -q https://downloads.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz\n",
        "    !tar -xzf kafka_2.13-3.7.0.tgz\n",
        "    !mv kafka_2.13-3.7.0 kafka\n",
        "\n",
        "# Start Zookeeper (background/daemon)\n",
        "!kafka/bin/zookeeper-server-start.sh -daemon kafka/config/zookeeper.properties\n",
        "print(\"Starting Zookeeper...\")\n",
        "time.sleep(10)  # give Zookeeper time to start\n",
        "\n",
        "# Start Kafka broker (background/daemon)\n",
        "!kafka/bin/kafka-server-start.sh -daemon kafka/config/server.properties\n",
        "print(\"Starting Kafka broker...\")\n",
        "time.sleep(15)  # give Kafka time to start\n",
        "\n",
        "print(\"Kafka cluster should now be running on localhost:9092.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e7ede9",
      "metadata": {
        "id": "39e7ede9"
      },
      "source": [
        "You can optionally check that the Kafka processes are running (they should contain `zookeeper` and `kafka.Kafka`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "292e637b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "292e637b",
        "outputId": "64441066-1602-4595-d242-8292d096a224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        1662  0.0  0.0   7376  3536 ?        S    18:05   0:00 /bin/bash -c ps aux | grep -E \"zookeeper|kafka.Kafka\" | head\n",
            "root        1664  0.0  0.0   6624  2388 ?        S    18:05   0:00 grep -E zookeeper|kafka.Kafka\n"
          ]
        }
      ],
      "source": [
        "# Optional: Check running Java processes (may show zookeeper + kafka)\n",
        "!ps aux | grep -E \"zookeeper|kafka.Kafka\" | head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58292036",
      "metadata": {
        "id": "58292036"
      },
      "source": [
        "## Task 2 â€“ Creating a Kafka Topic\n",
        "\n",
        "In the original lab, you opened a shell inside the Kafka container and ran:\n",
        "\n",
        "```bash\n",
        "kafka-topics --create --topic streaming-demo --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1\n",
        "kafka-topics --list --bootstrap-server localhost:9092\n",
        "```\n",
        "\n",
        "In this Colab version, we use the same Kafka CLI tools, but with the `.sh` suffix and the local `kafka` directory.\n",
        "\n",
        "We will:\n",
        "\n",
        "1. Create a topic named **`streaming-demo`** with 3 partitions and replication factor 1  \n",
        "2. List all topics to confirm it was created successfully  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eff45492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eff45492",
        "outputId": "7b830ea1-f120-4560-db0a-62229fc32417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: kafka/bin/kafka-topics.sh: No such file or directory\n",
            "/bin/bash: line 1: kafka/bin/kafka-topics.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Task 2 â€“ Create the 'streaming-demo' topic\n",
        "!kafka/bin/kafka-topics.sh --create   --topic streaming-demo   --bootstrap-server localhost:9092   --partitions 3   --replication-factor 1\n",
        "\n",
        "# Verify that the topic exists\n",
        "!kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fb5cbf",
      "metadata": {
        "id": "08fb5cbf"
      },
      "source": [
        "## Task 3 â€“ Creating a Kafka Producer (Message Publisher)\n",
        "\n",
        "In the original lab, you ran a producer inside the Kafka container and **typed messages manually**:\n",
        "\n",
        "```bash\n",
        "kafka-console-producer --topic streaming-demo --bootstrap-server localhost:9092\n",
        "\n",
        "Hello Kafka!\n",
        "Streaming data test.\n",
        "Another real-time message.\n",
        "```\n",
        "\n",
        "In this notebook, weâ€™ll do the same thing but **script the messages** so the lab is reproducible without manual typing.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Use `kafka-console-producer.sh` to publish a few test messages to the `streaming-demo` topic  \n",
        "- Treat these messages as our **synthetic streaming data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3ba85092",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ba85092",
        "outputId": "b53da639-6f68-47b8-bd7d-8fe4b10e9928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending messages to Kafka:\n",
            " Hello Kafka!\n",
            "Streaming data test.\n",
            "Another real-time message.\n",
            "Synthetic message 4.\n",
            "Synthetic message 5.\n",
            "\n",
            "/bin/bash: line 6: kafka/bin/kafka-console-producer.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Task 3 â€“ Send some synthetic messages to the 'streaming-demo' topic\n",
        "\n",
        "messages = \"\"\"Hello Kafka!\n",
        "Streaming data test.\n",
        "Another real-time message.\n",
        "Synthetic message 4.\n",
        "Synthetic message 5.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sending messages to Kafka:\\n\", messages)\n",
        "\n",
        "# Pipe the messages into the Kafka console producer\n",
        "# The producer will read from stdin and send each line as a message\n",
        "get_ipython().system('printf \"%s\" \"{messages}\" | kafka/bin/kafka-console-producer.sh --topic streaming-demo --bootstrap-server localhost:9092')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68246f90",
      "metadata": {
        "id": "68246f90"
      },
      "source": [
        "## Task 4 â€“ Creating a Kafka Consumer (Message Subscriber)\n",
        "\n",
        "In the original lab, you:\n",
        "\n",
        "1. Opened a second PowerShell window  \n",
        "2. Opened a bash shell inside the Kafka container  \n",
        "3. Ran a consumer from the beginning of the topic and watched the messages appear in real time:\n",
        "\n",
        "```bash\n",
        "kafka-console-consumer --topic streaming-demo --bootstrap-server localhost:9092 --from-beginning\n",
        "```\n",
        "\n",
        "In this Colab notebook, weâ€™ll do the same thing, but weâ€™ll add a **timeout** so the consumer doesnâ€™t run forever.\n",
        "\n",
        "We will:\n",
        "\n",
        "- Start a consumer with `--from-beginning` so it reads **all** messages from the topic  \n",
        "- Use `--timeout-ms` so the command exits after a short period of inactivity  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "54b9198f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b9198f",
        "outputId": "1d3913e1-52ad-48d4-d440-7bd5ebdd1453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consuming messages (from beginning)...\n",
            "/bin/bash: line 1: kafka/bin/kafka-console-consumer.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Task 4 â€“ Consume messages from the 'streaming-demo' topic\n",
        "\n",
        "print(\"Consuming messages (from beginning)...\")\n",
        "# --timeout-ms 5000 means: stop after 5 seconds with no new messages\n",
        "!kafka/bin/kafka-console-consumer.sh --topic streaming-demo --bootstrap-server localhost:9092 --from-beginning --timeout-ms 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fe77c56",
      "metadata": {
        "id": "0fe77c56"
      },
      "source": [
        "### 4.1 Add More Messages and Confirm They Appear\n",
        "\n",
        "In the original lab, you then typed additional lines in the producer window and verified that they showed up in the consumer.\n",
        "\n",
        "Here, we will:\n",
        "\n",
        "1. Send a few more synthetic messages  \n",
        "2. Run the consumer again from the beginning to see **all** messages in the topic (old + new)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "070d2c37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "070d2c37",
        "outputId": "7402434b-c601-4402-b7d4-23ffd0a5991b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending additional messages to Kafka:\n",
            " New message after initial batch.\n",
            "Real-time update 1.\n",
            "Real-time update 2.\n",
            "\n",
            "/bin/bash: line 4: kafka/bin/kafka-console-producer.sh: No such file or directory\n",
            "\n",
            "Re-consuming messages from the beginning (should include ALL messages so far):\n",
            "/bin/bash: line 1: kafka/bin/kafka-console-consumer.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Send a few additional messages\n",
        "more_messages = \"\"\"New message after initial batch.\n",
        "Real-time update 1.\n",
        "Real-time update 2.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Sending additional messages to Kafka:\\n\", more_messages)\n",
        "get_ipython().system('printf \"%s\" \"{more_messages}\" | kafka/bin/kafka-console-producer.sh --topic streaming-demo --bootstrap-server localhost:9092')\n",
        "\n",
        "print(\"\\nRe-consuming messages from the beginning (should include ALL messages so far):\")\n",
        "!kafka/bin/kafka-console-consumer.sh --topic streaming-demo --bootstrap-server localhost:9092 --from-beginning --timeout-ms 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3218fcba",
      "metadata": {
        "id": "3218fcba"
      },
      "source": [
        "## Task 5 â€“ Demonstrating Kafka Fault Tolerance and Scalability\n",
        "\n",
        "\n",
        "In this Colab we perform the following:\n",
        "\n",
        "- **Stopping the Kafka process** using `pkill`  \n",
        "- Trying to send new messages (this should fail or hang)  \n",
        "- Restarting the Kafka broker  \n",
        "- Re-consuming from the beginning to show that previously written messages are still available\n",
        "\n",
        "> âš ï¸ Note: In a production cluster, **fault tolerance** comes from having multiple brokers and a replication factor > 1.  \n",
        "> Here we are just demonstrating that **restarting the broker** does not lose already committed messages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19c55a4",
      "metadata": {
        "id": "d19c55a4"
      },
      "source": [
        "### 5.1 Stop the Kafka Broker\n",
        "\n",
        "We will stop the Kafka broker process but leave Zookeeper running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f0c54d4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0c54d4a",
        "outputId": "f16fdca9-af75-46b9-d418-4c26f97aa153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping Kafka broker...\n",
            "^C\n",
            "root        1998  0.0  0.0   7376  3456 ?        S    18:07   0:00 /bin/bash -c ps aux | grep -E \"zookeeper|kafka.Kafka\" | head\n",
            "root        2000  0.0  0.0   6624  2332 ?        S    18:07   0:00 grep -E zookeeper|kafka.Kafka\n"
          ]
        }
      ],
      "source": [
        "# Attempt to stop the Kafka broker process\n",
        "print(\"Stopping Kafka broker...\")\n",
        "!pkill -f kafka.Kafka || echo \"Kafka process not found (may already be stopped).\"\n",
        "\n",
        "# Check processes to confirm\n",
        "!ps aux | grep -E \"zookeeper|kafka.Kafka\" | head"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41927bdb",
      "metadata": {
        "id": "41927bdb"
      },
      "source": [
        "### 5.2 Try Sending Messages While Broker is Down\n",
        "\n",
        "Now we attempt to send additional messages with the producer.  \n",
        "This should **fail** or result in errors, because the Kafka broker is not running.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5a187d82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a187d82",
        "outputId": "c4718d88-62dd-4534-b971-12020d3488f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to send messages while Kafka broker is stopped (expect errors)...\n",
            "/bin/bash: line 3: kafka/bin/kafka-console-producer.sh: No such file or directory\n",
            "Producer failed as expected.\n"
          ]
        }
      ],
      "source": [
        "failed_messages = \"\"\"This message should fail 1.\n",
        "This message should fail 2.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Trying to send messages while Kafka broker is stopped (expect errors)...\")\n",
        "get_ipython().system('printf \"%s\" \"{failed_messages}\" | kafka/bin/kafka-console-producer.sh --topic streaming-demo --bootstrap-server localhost:9092 || echo \"Producer failed as expected.\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f463ce1c",
      "metadata": {
        "id": "f463ce1c"
      },
      "source": [
        "### 5.3 Restart Kafka Broker and Verify Message Retention\n",
        "\n",
        "Now we restart the Kafka broker and run the consumer again from the beginning.\n",
        "\n",
        "You should see that **all messages sent before the broker was stopped** are still present.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0b3a03dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3a03dd",
        "outputId": "6827a7af-9a12-4de5-813d-9c36b8e4d0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restarting Kafka broker...\n",
            "/bin/bash: line 1: kafka/bin/kafka-server-start.sh: No such file or directory\n",
            "Kafka broker restarted. Re-consuming from the beginning to verify retained messages:\n",
            "/bin/bash: line 1: kafka/bin/kafka-console-consumer.sh: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print(\"Restarting Kafka broker...\")\n",
        "!kafka/bin/kafka-server-start.sh -daemon kafka/config/server.properties\n",
        "time.sleep(15)\n",
        "\n",
        "print(\"Kafka broker restarted. Re-consuming from the beginning to verify retained messages:\")\n",
        "!kafka/bin/kafka-console-consumer.sh --topic streaming-demo --bootstrap-server localhost:9092 --from-beginning --timeout-ms 5000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9d8d74",
      "metadata": {
        "id": "2c9d8d74"
      },
      "source": [
        "## Lab Review\n",
        "\n",
        "1. **What happens if a Kafka broker fails? How does Kafka ensure fault tolerance?**  \n",
        "   - Think about **replication** and **partitions** across multiple brokers in a production cluster.  \n",
        "\n",
        "2. **Why does Kafka use partitions, and how does that improve scalability?**  \n",
        "   - A. It allows parallel processing across consumers  \n",
        "   - B. It ensures message ordering *within* partitions  \n",
        "   - C. Both A and B  \n",
        "\n",
        "In this Colab lab, you:\n",
        "- Set up a **single-node Kafka cluster** inside the notebook  \n",
        "- Created a **topic** with multiple partitions  \n",
        "- Used a **producer** to send streaming messages (synthetic data)  \n",
        "- Used a **consumer** to read messages from the beginning  \n",
        "- Simulated a **broker failure** and verified that previously committed messages were still available after restart  \n",
        "\n",
        "> âœ… You have successfully completed the Kafka streaming lab in Google Colab!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc80245d",
      "metadata": {
        "id": "dc80245d"
      },
      "source": [
        "### (Optional) Cleanup\n",
        "\n",
        "If you want to stop Kafka and Zookeeper at the end of the lab, you can run the following cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d40b7afe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d40b7afe",
        "outputId": "3b91142d-553d-462f-fb41-42ba9c97f844"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping Kafka and Zookeeper (optional cleanup)...\n",
            "^C\n",
            "^C\n",
            "root        2191  0.0  0.0   7376  3496 ?        S    18:07   0:00 /bin/bash -c ps aux | grep -E \"zookeeper|kafka.Kafka\" | head\n",
            "root        2193  0.0  0.0   6624  2568 ?        S    18:07   0:00 grep -E zookeeper|kafka.Kafka\n"
          ]
        }
      ],
      "source": [
        "# Optional cleanup: stop Kafka and Zookeeper\n",
        "print(\"Stopping Kafka and Zookeeper (optional cleanup)...\")\n",
        "!pkill -f kafka.Kafka || echo \"Kafka broker already stopped.\"\n",
        "!pkill -f zookeeper || echo \"Zookeeper already stopped.\"\n",
        "!ps aux | grep -E \"zookeeper|kafka.Kafka\" | head"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}