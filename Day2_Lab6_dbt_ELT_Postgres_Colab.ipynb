{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36c023cd",
      "metadata": {
        "id": "36c023cd"
      },
      "source": [
        "# Lab 6: Build an ELT Pipeline with dbt on Postgres (Google Colab Version)\n",
        "\n",
        "In this Colab we:\n",
        "\n",
        "- Install and start **Postgres** inside the Colab VM.\n",
        "- Install **dbt-core** and **dbt-postgres** via `pip`.\n",
        "- Programmatically create the dbt **project** and **profiles** instead of using interactive prompts.\n",
        "- Use Python and SQL from this notebook instead of pgAdmin.\n",
        "- Generate dbt **documentation** and show where to download it.\n",
        "\n",
        "> ğŸ’¡ **Goal:** Experience an end-to-end ELT pipeline where raw data is loaded into Postgres, transformed with dbt, tested, and documented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f478c3f5",
      "metadata": {
        "id": "f478c3f5"
      },
      "source": [
        "## Task 1 â€“ Setting up dbt and Postgres\n",
        "\n",
        "In this Colab notebook we:\n",
        "\n",
        "- We install **Postgres** and start the database server inside the Colab VM.\n",
        "- We install **dbt-core** and **dbt-postgres** with `pip`.\n",
        "- We programmatically create the **dbt project** folder and `profiles.yml` file with the correct Postgres settings.\n",
        "- Then we run `dbt debug` to verify connectivity.\n",
        "\n",
        "Run the following cell **once** at the top of your session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c620e081",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c620e081",
        "outputId": "809c451e-5639-41f3-aad6-8a2790a1722e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to security.ub\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,153 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,836 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,592 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,491 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,222 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,870 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,008 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,535 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,286 kB]\n",
            "Fetched 37.5 MB in 6s (5,807 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common ssl-cert\n",
            "  sysstat\n",
            "Suggested packages:\n",
            "  bsd-mailx | mailx postgresql-doc postgresql-doc-14 isag\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libllvm14\n",
            "  libtypes-serialiser-perl logrotate netbase postgresql postgresql-14\n",
            "  postgresql-client-14 postgresql-client-common postgresql-common\n",
            "  postgresql-contrib ssl-cert sysstat\n",
            "0 upgraded, 15 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 42.4 MB of archives.\n",
            "After this operation, 162 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 logrotate amd64 3.19.0-1ubuntu1.1 [54.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 netbase all 6.3 [12.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcommon-sense-perl amd64 3.75-2build1 [21.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjson-perl all 4.04000-1 [81.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtypes-serialiser-perl all 1.01-1 [11.6 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libjson-xs-perl amd64 4.040-0ubuntu0.22.04.1 [87.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm14 amd64 1:14.0.0-1ubuntu1.1 [24.0 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-client-common all 238 [29.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-client-14 amd64 14.19-0ubuntu0.22.04.1 [1,249 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 ssl-cert all 1.1.2 [17.4 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-common all 238 [169 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 postgresql-14 amd64 14.19-0ubuntu0.22.04.1 [16.2 MB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql all 14+238 [3,288 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 postgresql-contrib all 14+238 [3,292 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 sysstat amd64 12.5.2-2ubuntu0.2 [487 kB]\n",
            "Fetched 42.4 MB in 1s (31.1 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package logrotate.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../00-logrotate_3.19.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../01-netbase_6.3_all.deb ...\n",
            "Unpacking netbase (6.3) ...\n",
            "Selecting previously unselected package libcommon-sense-perl:amd64.\n",
            "Preparing to unpack .../02-libcommon-sense-perl_3.75-2build1_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../03-libjson-perl_4.04000-1_all.deb ...\n",
            "Unpacking libjson-perl (4.04000-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../04-libtypes-serialiser-perl_1.01-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.01-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../05-libjson-xs-perl_4.040-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libllvm14:amd64.\n",
            "Preparing to unpack .../06-libllvm14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../07-postgresql-client-common_238_all.deb ...\n",
            "Unpacking postgresql-client-common (238) ...\n",
            "Selecting previously unselected package postgresql-client-14.\n",
            "Preparing to unpack .../08-postgresql-client-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../09-ssl-cert_1.1.2_all.deb ...\n",
            "Unpacking ssl-cert (1.1.2) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../10-postgresql-common_238_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (238) ...\n",
            "Selecting previously unselected package postgresql-14.\n",
            "Preparing to unpack .../11-postgresql-14_14.19-0ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../12-postgresql_14+238_all.deb ...\n",
            "Unpacking postgresql (14+238) ...\n",
            "Selecting previously unselected package postgresql-contrib.\n",
            "Preparing to unpack .../13-postgresql-contrib_14+238_all.deb ...\n",
            "Unpacking postgresql-contrib (14+238) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../14-sysstat_12.5.2-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking sysstat (12.5.2-2ubuntu0.2) ...\n",
            "Setting up logrotate (3.19.0-1ubuntu1.1) ...\n",
            "Created symlink /etc/systemd/system/timers.target.wants/logrotate.timer â†’ /lib/systemd/system/logrotate.timer.\n",
            "Setting up libcommon-sense-perl:amd64 (3.75-2build1) ...\n",
            "Setting up ssl-cert (1.1.2) ...\n",
            "Setting up libllvm14:amd64 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libtypes-serialiser-perl (1.01-1) ...\n",
            "Setting up libjson-perl (4.04000-1) ...\n",
            "Setting up netbase (6.3) ...\n",
            "Setting up sysstat (12.5.2-2ubuntu0.2) ...\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-collect.timer â†’ /lib/systemd/system/sysstat-collect.timer.\n",
            "Created symlink /etc/systemd/system/sysstat.service.wants/sysstat-summary.timer â†’ /lib/systemd/system/sysstat-summary.timer.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service â†’ /lib/systemd/system/sysstat.service.\n",
            "Setting up postgresql-client-common (238) ...\n",
            "Setting up libjson-xs-perl (4.040-0ubuntu0.22.04.1) ...\n",
            "Setting up postgresql-client-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-common (238) ...\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service â†’ /lib/systemd/system/postgresql.service.\n",
            "Setting up postgresql-14 (14.19-0ubuntu0.22.04.1) ...\n",
            "Creating new PostgreSQL cluster 14/main ...\n",
            "/usr/lib/postgresql/14/bin/initdb -D /var/lib/postgresql/14/main --auth-local peer --auth-host scram-sha-256 --no-instructions\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/14/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default time zone ... Etc/UTC\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "update-alternatives: using /usr/share/postgresql/14/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-contrib (14+238) ...\n",
            "Setting up postgresql (14+238) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.4/114.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m986.2/986.2 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.9/144.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.7/442.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.0/147.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.4/91.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.1 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Task 1 â€“ Install Postgres and dbt, and start the Postgres service\n",
        "# This may take a few minutes the first time it runs.\n",
        "\n",
        "import os, time\n",
        "\n",
        "# Install Postgres\n",
        "!apt-get -y update\n",
        "!apt-get -y install postgresql postgresql-contrib\n",
        "\n",
        "# Start the PostgreSQL service\n",
        "!service postgresql start\n",
        "\n",
        "# Set a password for the 'postgres' superuser so we can connect via TCP\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "\n",
        "# Install dbt-core and dbt-postgres\n",
        "!pip install -q dbt-core dbt-postgres psycopg2-binary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f3c014",
      "metadata": {
        "id": "24f3c014"
      },
      "source": [
        "### Connect to Postgres from Python\n",
        "\n",
        "In the original lab, you work with Postgres via **pgAdmin**. In this notebook, we will:\n",
        "\n",
        "1. Connect to the `postgres` database as the `postgres` superuser.\n",
        "2. Keep a reusable connection and cursor.\n",
        "3. Define a helper function `run_sql` to execute SQL commands and optionally print results.\n",
        "\n",
        "> ğŸ” If you restart the runtime, re-run the previous cell and this one to re-establish the connection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "104b25ff",
      "metadata": {
        "id": "104b25ff"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "\n",
        "# Connect to the default 'postgres' database over TCP using the password we set above.\n",
        "conn = psycopg2.connect(\n",
        "    dbname=\"postgres\",\n",
        "    user=\"postgres\",\n",
        "    password=\"postgres\",\n",
        "    host=\"127.0.0.1\",\n",
        "    port=5432,\n",
        ")\n",
        "conn.autocommit = True\n",
        "cur = conn.cursor()\n",
        "\n",
        "def run_sql(sql, fetch=False, max_rows=20):\n",
        "    \"\"\"Helper to run SQL against PostgreSQL and optionally print results.\n",
        "\n",
        "    Args:\n",
        "        sql (str): SQL command(s) to execute.\n",
        "        fetch (bool): If True, fetch and print results.\n",
        "        max_rows (int): Max number of rows to display when fetching.\n",
        "    \"\"\"\n",
        "    print(\"Running SQL:\\n\" + \"-\" * 80)\n",
        "    print(sql.strip())\n",
        "    print(\"-\" * 80)\n",
        "    cur.execute(sql)\n",
        "\n",
        "    if fetch:\n",
        "        rows = cur.fetchall()\n",
        "        print(f\"Returned {len(rows)} rows (showing up to {max_rows}):\")\n",
        "        for row in rows[:max_rows]:\n",
        "            print(row)\n",
        "    else:\n",
        "        try:\n",
        "            print(f\"Rowcount: {cur.rowcount}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca70afb",
      "metadata": {
        "id": "fca70afb"
      },
      "source": [
        "### Creating the dbt Project and Profile\n",
        "\n",
        "In the original lab, `dbt init postgres_elt_pipeline` prompts you for:\n",
        "\n",
        "- **Adapter**: postgres  \n",
        "- **Host**: e.g., `sql-server` or IP  \n",
        "- **Port**: `5432`  \n",
        "- **User**: `postgres`  \n",
        "- **Password**: `Passw0rd!`  \n",
        "- **Database**: `postgres`  \n",
        "- **Schema**: `public`  \n",
        "- **Threads**: `4`  \n",
        "\n",
        "Here in Colab we:\n",
        "\n",
        "1. Create a folder `postgres_elt_pipeline` for the dbt project.\n",
        "2. Create `dbt_project.yml` with basic project metadata.\n",
        "3. Create `~/.dbt/profiles.yml` that dbt uses to connect to Postgres running in this notebook.\n",
        "4. Use host `127.0.0.1`, user `postgres`, password `postgres` (set earlier), DB `postgres`, schema `public`.\n",
        "\n",
        "Run the following cell to set this up.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cafe22b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cafe22b8",
        "outputId": "531d4244-ee72-488b-b150-8554617d12cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dbt project at: /content/postgres_elt_pipeline\n",
            "Created profiles.yml at: /root/.dbt/profiles.yml\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "project_dir = pathlib.Path(\"postgres_elt_pipeline\")\n",
        "models_dir = project_dir / \"models\"\n",
        "\n",
        "# Create project and models directories\n",
        "models_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# dbt project configuration (equivalent to dbt init output)\n",
        "dbt_project_yml = \"\"\"name: 'postgres_elt_pipeline'\n",
        "version: '1.0.0'\n",
        "profile: 'postgres_elt_pipeline'\n",
        "\n",
        "models:\n",
        "  postgres_elt_pipeline:\n",
        "    +schema: public\n",
        "    +materialized: view\n",
        "\"\"\"\n",
        "\n",
        "with open(project_dir / \"dbt_project.yml\", \"w\") as f:\n",
        "    f.write(dbt_project_yml)\n",
        "\n",
        "# dbt profiles configuration under ~/.dbt/profiles.yml\n",
        "profiles_dir = pathlib.Path.home() / \".dbt\"\n",
        "profiles_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "profiles_yml = \"\"\"postgres_elt_pipeline:\n",
        "  outputs:\n",
        "    dev:\n",
        "      type: postgres\n",
        "      host: 127.0.0.1\n",
        "      user: postgres\n",
        "      password: postgres\n",
        "      port: 5432\n",
        "      dbname: postgres\n",
        "      schema: public\n",
        "      threads: 4\n",
        "  target: dev\n",
        "\"\"\"\n",
        "\n",
        "with open(profiles_dir / \"profiles.yml\", \"w\") as f:\n",
        "    f.write(profiles_yml)\n",
        "\n",
        "print(\"Created dbt project at:\", project_dir.resolve())\n",
        "print(\"Created profiles.yml at:\", profiles_dir / \"profiles.yml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0151ed18",
      "metadata": {
        "id": "0151ed18"
      },
      "source": [
        "### Verifying the dbt Connection (`dbt debug`)\n",
        "\n",
        "In the original lab, you run:\n",
        "\n",
        "```bash\n",
        "dbt debug\n",
        "```\n",
        "\n",
        "to confirm that dbt can connect to Postgres using the settings in `profiles.yml`.\n",
        "\n",
        "We do the same here, but make sure to run dbt from inside the `postgres_elt_pipeline` project directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "489ea8eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "489ea8eb",
        "outputId": "ccd8e856-eaa3-4a46-bb5c-858a484edb05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m18:33:10  Running with dbt=1.10.15\n",
            "\u001b[0m18:33:10  dbt version: 1.10.15\n",
            "\u001b[0m18:33:10  python version: 3.12.12\n",
            "\u001b[0m18:33:10  python path: /usr/bin/python3\n",
            "\u001b[0m18:33:10  os info: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "\u001b[0m18:33:10  Using profiles dir at /root/.dbt\n",
            "\u001b[0m18:33:10  Using profiles.yml file at /root/.dbt/profiles.yml\n",
            "\u001b[0m18:33:10  Using dbt_project.yml file at /content/postgres_elt_pipeline/dbt_project.yml\n",
            "\u001b[0m18:33:10  adapter type: postgres\n",
            "\u001b[0m18:33:10  adapter version: 1.9.1\n",
            "\u001b[0m18:33:10  Configuration:\n",
            "\u001b[0m18:33:10    profiles.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
            "\u001b[0m18:33:10    dbt_project.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
            "\u001b[0m18:33:10  Required dependencies:\n",
            "\u001b[0m18:33:10   - git [\u001b[32mOK found\u001b[0m]\n",
            "\n",
            "\u001b[0m18:33:10  Connection:\n",
            "\u001b[0m18:33:10    host: 127.0.0.1\n",
            "\u001b[0m18:33:10    port: 5432\n",
            "\u001b[0m18:33:10    user: postgres\n",
            "\u001b[0m18:33:10    database: postgres\n",
            "\u001b[0m18:33:10    schema: public\n",
            "\u001b[0m18:33:10    connect_timeout: 10\n",
            "\u001b[0m18:33:10    role: None\n",
            "\u001b[0m18:33:10    search_path: None\n",
            "\u001b[0m18:33:10    keepalives_idle: 0\n",
            "\u001b[0m18:33:10    sslmode: None\n",
            "\u001b[0m18:33:10    sslcert: None\n",
            "\u001b[0m18:33:10    sslkey: None\n",
            "\u001b[0m18:33:10    sslrootcert: None\n",
            "\u001b[0m18:33:10    application_name: dbt\n",
            "\u001b[0m18:33:10    retries: 1\n",
            "\u001b[0m18:33:10  Registered adapter: postgres=1.9.1\n",
            "\u001b[0m18:33:10    Connection test: [\u001b[32mOK connection ok\u001b[0m]\n",
            "\n",
            "\u001b[0m18:33:10  \u001b[32mAll checks passed!\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Run dbt debug from the project directory to test the connection\n",
        "!cd postgres_elt_pipeline && dbt debug"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d283191c",
      "metadata": {
        "id": "d283191c"
      },
      "source": [
        "## Task 2 â€“ Loading Raw Data into Postgres\n",
        "\n",
        "In the original lab, you:\n",
        "\n",
        "1. Opened **pgAdmin 4** and connected to the `postgres` database.\n",
        "2. Created a table `public.raw_orders` using the Query Tool.\n",
        "3. Inserted several sample rows (including some imperfect data that will be cleaned later).\n",
        "4. Ran `SELECT * FROM raw_orders;` to verify.\n",
        "\n",
        "In this notebook, we will execute the same SQL commands directly from Python using our `run_sql` helper.\n",
        "\n",
        "### 2.1 Create the `raw_orders` Table\n",
        "\n",
        "The table definition mirrors the lab:\n",
        "\n",
        "```sql\n",
        "CREATE TABLE public.raw_orders (\n",
        "  order_id INT PRIMARY KEY,\n",
        "  customer_name VARCHAR(100),\n",
        "  product VARCHAR(100),\n",
        "  quantity INT,\n",
        "  price DECIMAL(10,2),\n",
        "  order_date TIMESTAMP\n",
        ");\n",
        "```\n",
        "\n",
        "Run the next cell to create the table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5ca79b28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ca79b28",
        "outputId": "ce8f5488-ec42-495e-c705-a26771998769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "DROP TABLE IF EXISTS public.raw_orders;\n",
            "\n",
            "CREATE TABLE public.raw_orders (\n",
            "  order_id INT PRIMARY KEY,\n",
            "  customer_name VARCHAR(100),\n",
            "  product VARCHAR(100),\n",
            "  quantity INT,\n",
            "  price DECIMAL(10,2),\n",
            "  order_date TIMESTAMP\n",
            ");\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: -1\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "create_table_sql = \"\"\"\n",
        "DROP TABLE IF EXISTS public.raw_orders;\n",
        "\n",
        "CREATE TABLE public.raw_orders (\n",
        "  order_id INT PRIMARY KEY,\n",
        "  customer_name VARCHAR(100),\n",
        "  product VARCHAR(100),\n",
        "  quantity INT,\n",
        "  price DECIMAL(10,2),\n",
        "  order_date TIMESTAMP\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "run_sql(create_table_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c419df5",
      "metadata": {
        "id": "3c419df5"
      },
      "source": [
        "### 2.2 Insert Sample Raw Data\n",
        "\n",
        "The original lab inserts a small set of orders, including:\n",
        "\n",
        "- Valid orders (e.g., Alice buying a Laptop).\n",
        "- Some **dirty data**, like:\n",
        "  - A row with **empty product** and **zero quantity**.\n",
        "  - Rows where `quantity` is provided as a string.\n",
        "\n",
        "We insert the same data here (with a tiny syntax fix: adding a missing comma between rows 4 and 5).\n",
        "\n",
        "Run the next cell to insert the sample data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "eafad37b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eafad37b",
        "outputId": "2037c255-99aa-4d0a-be4e-76a3b724443f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "INSERT INTO public.raw_orders (order_id, customer_name, product, quantity, price, order_date) VALUES\n",
            "(1, 'Alice',   'Laptop',     1, 1200.00, '2024-02-01 10:15:00'),\n",
            "(2, 'Bob',     'Mouse',      2,   25.00, '2024-02-02 12:30:00'),\n",
            "(3, 'Charlie', 'Keyboard',   1,   50.00, '2024-02-03 14:45:00'),\n",
            "(4, 'David',   'Monitor',    1,  300.00, '2024-02-04 16:00:00'),\n",
            "(5, 'Lewis',   '',           0,    0.00, '2024-09-11 14:23:30'),\n",
            "(6, 'Sally',   'Server',     3, 2100.00, '2024-09-11 14:14:22'),\n",
            "(7, 'Claire',  'Disk Drive', 2,  172.00, '2024-09-23 11:23:15');\n",
            "--------------------------------------------------------------------------------\n",
            "Rowcount: 7\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "insert_data_sql = \"\"\"\n",
        "INSERT INTO public.raw_orders (order_id, customer_name, product, quantity, price, order_date) VALUES\n",
        "(1, 'Alice',   'Laptop',     1, 1200.00, '2024-02-01 10:15:00'),\n",
        "(2, 'Bob',     'Mouse',      2,   25.00, '2024-02-02 12:30:00'),\n",
        "(3, 'Charlie', 'Keyboard',   1,   50.00, '2024-02-03 14:45:00'),\n",
        "(4, 'David',   'Monitor',    1,  300.00, '2024-02-04 16:00:00'),\n",
        "(5, 'Lewis',   '',           0,    0.00, '2024-09-11 14:23:30'),\n",
        "(6, 'Sally',   'Server',     3, 2100.00, '2024-09-11 14:14:22'),\n",
        "(7, 'Claire',  'Disk Drive', 2,  172.00, '2024-09-23 11:23:15');\n",
        "\"\"\"\n",
        "\n",
        "run_sql(insert_data_sql)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd01fa16",
      "metadata": {
        "id": "fd01fa16"
      },
      "source": [
        "### 2.3 Verify the Loaded Raw Data\n",
        "\n",
        "Just like running `SELECT * FROM raw_orders;` in pgAdmin, weâ€™ll query the table here to confirm the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2cc4c6cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc4c6cc",
        "outputId": "acfadad8-5294-41ca-830e-864a8cdc4cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT * FROM public.raw_orders;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 7 rows (showing up to 10):\n",
            "(1, 'Alice', 'Laptop', 1, Decimal('1200.00'), datetime.datetime(2024, 2, 1, 10, 15))\n",
            "(2, 'Bob', 'Mouse', 2, Decimal('25.00'), datetime.datetime(2024, 2, 2, 12, 30))\n",
            "(3, 'Charlie', 'Keyboard', 1, Decimal('50.00'), datetime.datetime(2024, 2, 3, 14, 45))\n",
            "(4, 'David', 'Monitor', 1, Decimal('300.00'), datetime.datetime(2024, 2, 4, 16, 0))\n",
            "(5, 'Lewis', '', 0, Decimal('0.00'), datetime.datetime(2024, 9, 11, 14, 23, 30))\n",
            "(6, 'Sally', 'Server', 3, Decimal('2100.00'), datetime.datetime(2024, 9, 11, 14, 14, 22))\n",
            "(7, 'Claire', 'Disk Drive', 2, Decimal('172.00'), datetime.datetime(2024, 9, 23, 11, 23, 15))\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_sql(\"SELECT * FROM public.raw_orders;\", fetch=True, max_rows=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d733559",
      "metadata": {
        "id": "9d733559"
      },
      "source": [
        "## Task 3 â€“ Defining and Executing dbt Transformations\n",
        "\n",
        "In the original lab, you create a dbt model file:\n",
        "\n",
        "`C:\\\\Users\\\\student\\\\postgres_elt_pipeline\\\\models\\\\clean_orders.sql`\n",
        "\n",
        "with the following logic:\n",
        "\n",
        "- Select from `{{ source('public', 'raw_orders') }}`.\n",
        "- Clean customer names to lower-case.\n",
        "- Compute a new `total_price` as `quantity * price`.\n",
        "- Strip the time component from `order_date` and cast to `DATE`.\n",
        "- Filter out rows where `quantity <= 0`.\n",
        "\n",
        "### 3.1 Create `models/clean_orders.sql`\n",
        "\n",
        "Weâ€™ll now create the same model file inside our project folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ba31ea01",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba31ea01",
        "outputId": "9e746bc3-5eb7-4048-be57-c88bacbcc1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote clean_orders model to: /content/postgres_elt_pipeline/models/clean_orders.sql\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "clean_model_sql = \"\"\"WITH cleaned AS (\n",
        "  SELECT\n",
        "    order_id,\n",
        "    LOWER(customer_name) AS customer_name,\n",
        "    product,\n",
        "    quantity,\n",
        "    price,\n",
        "    quantity * price AS total_price,\n",
        "    order_date::DATE AS order_date\n",
        "  FROM {{ source('public', 'raw_orders') }}\n",
        "  WHERE quantity > 0\n",
        ")\n",
        "SELECT * FROM cleaned\n",
        "\"\"\"\n",
        "\n",
        "clean_model_path = Path(\"postgres_elt_pipeline\") / \"models\" / \"clean_orders.sql\"\n",
        "with open(clean_model_path, \"w\") as f:\n",
        "    f.write(clean_model_sql)\n",
        "\n",
        "print(\"Wrote clean_orders model to:\", clean_model_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d415414",
      "metadata": {
        "id": "9d415414"
      },
      "source": [
        "## Task 4 â€“ Verifying Transformations and Testing Data Integrity\n",
        "\n",
        "In the original lab, you:\n",
        "\n",
        "1. Create `models/schema.yml` and define the **source** `public.raw_orders`.\n",
        "2. Delete contents of the `models/example` folder.\n",
        "3. Run `dbt run` to build the `clean_orders` view in Postgres.\n",
        "4. Confirm the view exists via pgAdmin.\n",
        "5. Extend `schema.yml` with **tests** (unique, not_null) for `clean_orders` columns.\n",
        "6. Run `dbt test` to validate data integrity.\n",
        "\n",
        "Weâ€™ll follow the same structure here.\n",
        "\n",
        "### 4.1 Define the Source in `models/schema.yml`\n",
        "\n",
        "First, we declare `public.raw_orders` as a dbt **source**, so that `{{ source('public', 'raw_orders') }}` in `clean_orders.sql` is recognized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "79b8abee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79b8abee",
        "outputId": "a1d20def-534f-411a-810e-e54de9050be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote initial schema.yml with sources to: /content/postgres_elt_pipeline/models/schema.yml\n"
          ]
        }
      ],
      "source": [
        "schema_yml_sources = \"\"\"version: 2\n",
        "\n",
        "sources:\n",
        "  - name: public\n",
        "    database: postgres\n",
        "    tables:\n",
        "      - name: raw_orders\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "schema_path = Path(\"postgres_elt_pipeline\") / \"models\" / \"schema.yml\"\n",
        "with open(schema_path, \"w\") as f:\n",
        "    f.write(schema_yml_sources)\n",
        "\n",
        "print(\"Wrote initial schema.yml with sources to:\", schema_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319a89c6",
      "metadata": {
        "id": "319a89c6"
      },
      "source": [
        "### 4.2 Remove Example Models (if present)\n",
        "\n",
        "The original lab asks you to delete the contents of the `models/example` folder so only your models remain.\n",
        "\n",
        "Our manually-created project does not include example models by default, but weâ€™ll still run a cleanup command that removes an `example` folder if it exists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "60a0a1a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60a0a1a3",
        "outputId": "2e9c22de-1f85-4ae6-e4ed-94a2b723e7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example models (if any) have been removed.\n"
          ]
        }
      ],
      "source": [
        "# Remove the example models folder if it exists (safe no-op otherwise)\n",
        "!rm -rf postgres_elt_pipeline/models/example\n",
        "print(\"Example models (if any) have been removed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d528b9",
      "metadata": {
        "id": "f5d528b9"
      },
      "source": [
        "### 4.3 Run dbt Transformations (`dbt run`)\n",
        "\n",
        "Now we run:\n",
        "\n",
        "```bash\n",
        "dbt run\n",
        "```\n",
        "\n",
        "from within the project directory. This should create a **view** named `clean_orders` in the `public` schema, based on our model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d2868167",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2868167",
        "outputId": "ed306782-2fea-4db8-d2ec-f8430c8df959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m18:34:44  Running with dbt=1.10.15\n",
            "\u001b[0m18:34:44  Registered adapter: postgres=1.9.1\n",
            "\u001b[0m18:34:45  Unable to do partial parsing because saved manifest not found. Starting full parse.\n",
            "\u001b[0m18:34:46  Found 1 model, 1 source, 459 macros\n",
            "\u001b[0m18:34:46  \n",
            "\u001b[0m18:34:46  Concurrency: 4 threads (target='dev')\n",
            "\u001b[0m18:34:46  \n",
            "\u001b[0m18:34:47  1 of 1 START sql view model public_public.clean_orders ......................... [RUN]\n",
            "\u001b[0m18:34:47  1 of 1 OK created sql view model public_public.clean_orders .................... [\u001b[32mCREATE VIEW\u001b[0m in 0.15s]\n",
            "\u001b[0m18:34:47  \n",
            "\u001b[0m18:34:47  Finished running 1 view model in 0 hours 0 minutes and 0.39 seconds (0.39s).\n",
            "\u001b[0m18:34:47  \n",
            "\u001b[0m18:34:47  \u001b[32mCompleted successfully\u001b[0m\n",
            "\u001b[0m18:34:47  \n",
            "\u001b[0m18:34:47  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1\n"
          ]
        }
      ],
      "source": [
        "# Run dbt to build the clean_orders view\n",
        "!cd postgres_elt_pipeline && dbt run"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bbc3261",
      "metadata": {
        "id": "8bbc3261"
      },
      "source": [
        "### 4.4 Verify the `clean_orders` View in Postgres\n",
        "\n",
        "Instead of checking pgAdmin, we query the view directly from this notebook.  \n",
        "You should see:\n",
        "\n",
        "- `customer_name` in lower-case.\n",
        "- `total_price` as `quantity * price`.\n",
        "- `order_date` with the time stripped off (DATE only).\n",
        "- Rows with `quantity <= 0` removed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b828b187",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b828b187",
        "outputId": "624a0a53-efe9-43be-bd1b-60f269fd2216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT * FROM public_public.clean_orders ORDER BY order_id;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 6 rows (showing up to 20):\n",
            "(1, 'alice', 'Laptop', 1, Decimal('1200.00'), Decimal('1200.00'), datetime.date(2024, 2, 1))\n",
            "(2, 'bob', 'Mouse', 2, Decimal('25.00'), Decimal('50.00'), datetime.date(2024, 2, 2))\n",
            "(3, 'charlie', 'Keyboard', 1, Decimal('50.00'), Decimal('50.00'), datetime.date(2024, 2, 3))\n",
            "(4, 'david', 'Monitor', 1, Decimal('300.00'), Decimal('300.00'), datetime.date(2024, 2, 4))\n",
            "(6, 'sally', 'Server', 3, Decimal('2100.00'), Decimal('6300.00'), datetime.date(2024, 9, 11))\n",
            "(7, 'claire', 'Disk Drive', 2, Decimal('172.00'), Decimal('344.00'), datetime.date(2024, 9, 23))\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_sql(\"SELECT * FROM public_public.clean_orders ORDER BY order_id;\", fetch=True, max_rows=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80607bdf",
      "metadata": {
        "id": "80607bdf"
      },
      "source": [
        "### 4.5 Add Tests to `schema.yml`\n",
        "\n",
        "The original lab extends `schema.yml` with a `models:` section to add:\n",
        "\n",
        "- A **unique** and **not_null** test on `order_id`.\n",
        "- A **not_null** test on `quantity`.\n",
        "\n",
        "Weâ€™ll now update `models/schema.yml` to include both the **sources** and the **model tests**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6aa1df2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aa1df2c",
        "outputId": "01479ab4-158f-420a-efa3-e53c223eac55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated schema.yml with model tests at: /content/postgres_elt_pipeline/models/schema.yml\n"
          ]
        }
      ],
      "source": [
        "schema_yml_full = \"\"\"version: 2\n",
        "\n",
        "sources:\n",
        "  - name: public\n",
        "    database: postgres\n",
        "    tables:\n",
        "      - name: raw_orders\n",
        "\n",
        "models:\n",
        "  - name: clean_orders\n",
        "    description: \"Cleaned customer order data\"\n",
        "    columns:\n",
        "      - name: order_id\n",
        "        tests:\n",
        "          - unique\n",
        "          - not_null\n",
        "      - name: quantity\n",
        "        tests:\n",
        "          - not_null\n",
        "\"\"\"\n",
        "\n",
        "with open(schema_path, \"w\") as f:\n",
        "    f.write(schema_yml_full)\n",
        "\n",
        "print(\"Updated schema.yml with model tests at:\", schema_path.resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8487d5a5",
      "metadata": {
        "id": "8487d5a5"
      },
      "source": [
        "### 4.6 Run dbt Tests (`dbt test`)\n",
        "\n",
        "Now we run:\n",
        "\n",
        "```bash\n",
        "dbt test\n",
        "```\n",
        "\n",
        "This executes the tests defined in `schema.yml`, checking:\n",
        "\n",
        "- `clean_orders.order_id` is **unique** and **not null**.\n",
        "- `clean_orders.quantity` is **not null**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cf1f8d8f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf1f8d8f",
        "outputId": "4f25eb10-eba7-4891-c111-28b7d731cd00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m18:36:20  Running with dbt=1.10.15\n",
            "\u001b[0m18:36:20  Registered adapter: postgres=1.9.1\n",
            "\u001b[0m18:36:21  Found 1 model, 3 data tests, 1 source, 459 macros\n",
            "\u001b[0m18:36:21  \n",
            "\u001b[0m18:36:21  Concurrency: 4 threads (target='dev')\n",
            "\u001b[0m18:36:21  \n",
            "\u001b[0m18:36:21  1 of 3 START test not_null_clean_orders_order_id ............................... [RUN]\n",
            "\u001b[0m18:36:21  2 of 3 START test not_null_clean_orders_quantity ............................... [RUN]\n",
            "\u001b[0m18:36:21  3 of 3 START test unique_clean_orders_order_id ................................. [RUN]\n",
            "\u001b[0m18:36:21  3 of 3 PASS unique_clean_orders_order_id ....................................... [\u001b[32mPASS\u001b[0m in 0.17s]\n",
            "\u001b[0m18:36:21  2 of 3 PASS not_null_clean_orders_quantity ..................................... [\u001b[32mPASS\u001b[0m in 0.17s]\n",
            "\u001b[0m18:36:21  1 of 3 PASS not_null_clean_orders_order_id ..................................... [\u001b[32mPASS\u001b[0m in 0.18s]\n",
            "\u001b[0m18:36:21  \n",
            "\u001b[0m18:36:21  Finished running 3 data tests in 0 hours 0 minutes and 0.34 seconds (0.34s).\n",
            "\u001b[0m18:36:21  \n",
            "\u001b[0m18:36:21  \u001b[32mCompleted successfully\u001b[0m\n",
            "\u001b[0m18:36:21  \n",
            "\u001b[0m18:36:21  Done. PASS=3 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=3\n"
          ]
        }
      ],
      "source": [
        "# Run dbt tests\n",
        "!cd postgres_elt_pipeline && dbt test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbcd4b43",
      "metadata": {
        "id": "fbcd4b43"
      },
      "source": [
        "## Task 5 â€“ Documenting and Reviewing Changes\n",
        "\n",
        "In the original lab, you:\n",
        "\n",
        "1. Run `dbt docs generate` to build documentation.\n",
        "2. Run `dbt docs serve` to start a local web server and view docs in the browser.\n",
        "3. Navigate to `clean_orders` in the **Database** tab to inspect the model and SQL.\n",
        "\n",
        "In this Colab environment, we will:\n",
        "\n",
        "- Run `dbt docs generate` to build the docs into the `target/` folder.\n",
        "- Inspect the generated files.\n",
        "- (Optionally) download the `target` folder or `index.html` to view locally.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "07bafc87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07bafc87",
        "outputId": "3e2ea4c6-3af2-48cb-f414-6df23bc9184b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m18:36:35  Running with dbt=1.10.15\n",
            "\u001b[0m18:36:35  Registered adapter: postgres=1.9.1\n",
            "\u001b[0m18:36:36  Found 1 model, 3 data tests, 1 source, 459 macros\n",
            "\u001b[0m18:36:36  \n",
            "\u001b[0m18:36:36  Concurrency: 4 threads (target='dev')\n",
            "\u001b[0m18:36:36  \n",
            "\u001b[0m18:36:36  Building catalog\n",
            "\u001b[0m18:36:36  Catalog written to /content/postgres_elt_pipeline/target/catalog.json\n"
          ]
        }
      ],
      "source": [
        "# Generate dbt docs\n",
        "!cd postgres_elt_pipeline && dbt docs generate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea242fb2",
      "metadata": {
        "id": "ea242fb2"
      },
      "source": [
        "### 5.1 Inspect Generated Docs\n",
        "\n",
        "Locally, you would next run `dbt docs serve` to launch a browser-based UI.  \n",
        "Running a long-lived web server process is not ideal in Colab, so instead we:\n",
        "\n",
        "- List the generated files in the `target` directory.\n",
        "- Note that you can **download** `target/index.html` and open it in your browser to view the docs UI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "88936740",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88936740",
        "outputId": "6a5a9ea0-ff70-46aa-e34e-0b1a3a377192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "postgres_elt_pipeline/target:\n",
            "catalog.json\t    index.html\t\t   run_results.json\n",
            "compiled\t    manifest.json\t   semantic_manifest.json\n",
            "graph.gpickle\t    partial_parse.msgpack\n",
            "graph_summary.json  run\n",
            "\n",
            "postgres_elt_pipeline/target/compiled:\n",
            "postgres_elt_pipeline\n",
            "\n",
            "postgres_elt_pipeline/target/compiled/postgres_elt_pipeline:\n",
            "models\n",
            "\n",
            "postgres_elt_pipeline/target/compiled/postgres_elt_pipeline/models:\n",
            "clean_orders.sql  schema.yml\n",
            "\n",
            "postgres_elt_pipeline/target/compiled/postgres_elt_pipeline/models/schema.yml:\n",
            "not_null_clean_orders_order_id.sql  unique_clean_orders_order_id.sql\n",
            "not_null_clean_orders_quantity.sql\n",
            "\n",
            "postgres_elt_pipeline/target/run:\n",
            "postgres_elt_pipeline\n",
            "\n",
            "postgres_elt_pipeline/target/run/postgres_elt_pipeline:\n",
            "models\n",
            "\n",
            "postgres_elt_pipeline/target/run/postgres_elt_pipeline/models:\n",
            "clean_orders.sql  schema.yml\n",
            "\n",
            "postgres_elt_pipeline/target/run/postgres_elt_pipeline/models/schema.yml:\n",
            "not_null_clean_orders_order_id.sql  unique_clean_orders_order_id.sql\n",
            "not_null_clean_orders_quantity.sql\n",
            "\n",
            "To view the documentation UI, download 'postgres_elt_pipeline/target/index.html' and open it in a local browser.\n"
          ]
        }
      ],
      "source": [
        "# List the generated documentation files\n",
        "!ls -R postgres_elt_pipeline/target\n",
        "\n",
        "print(\"\\nTo view the documentation UI, download 'postgres_elt_pipeline/target/index.html' and open it in a local browser.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178c2dd3",
      "metadata": {
        "id": "178c2dd3"
      },
      "source": [
        "### 5.2 Query `clean_orders` One More Time\n",
        "\n",
        "As a final verification (similar to the last step in the lab where you query `clean_orders` from pgAdmin), we run:\n",
        "\n",
        "```sql\n",
        "SELECT * FROM clean_orders;\n",
        "```\n",
        "\n",
        "to confirm:\n",
        "\n",
        "- `total_price` reflects `quantity * price`.\n",
        "- `order_date` is a `DATE` (no time component).\n",
        "- `customer_name` is lower-case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f6a44bca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6a44bca",
        "outputId": "59e45f76-d53e-4bc5-993a-c072897d6590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running SQL:\n",
            "--------------------------------------------------------------------------------\n",
            "SELECT * FROM public_public.clean_orders ORDER BY order_id;\n",
            "--------------------------------------------------------------------------------\n",
            "Returned 6 rows (showing up to 20):\n",
            "(1, 'alice', 'Laptop', 1, Decimal('1200.00'), Decimal('1200.00'), datetime.date(2024, 2, 1))\n",
            "(2, 'bob', 'Mouse', 2, Decimal('25.00'), Decimal('50.00'), datetime.date(2024, 2, 2))\n",
            "(3, 'charlie', 'Keyboard', 1, Decimal('50.00'), Decimal('50.00'), datetime.date(2024, 2, 3))\n",
            "(4, 'david', 'Monitor', 1, Decimal('300.00'), Decimal('300.00'), datetime.date(2024, 2, 4))\n",
            "(6, 'sally', 'Server', 3, Decimal('2100.00'), Decimal('6300.00'), datetime.date(2024, 9, 11))\n",
            "(7, 'claire', 'Disk Drive', 2, Decimal('172.00'), Decimal('344.00'), datetime.date(2024, 9, 23))\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "run_sql(\"SELECT * FROM public_public.clean_orders ORDER BY order_id;\", fetch=True, max_rows=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0588b813",
      "metadata": {
        "id": "0588b813"
      },
      "source": [
        "## Wrap-Up and Cleanup\n",
        "\n",
        "You have now:\n",
        "\n",
        "- Set up Postgres and dbt in a Colab environment.\n",
        "- Loaded raw order data into Postgres.\n",
        "- Created a dbt model to clean and transform the data.\n",
        "- Added tests to validate data integrity.\n",
        "- Generated dbt documentation for the project.\n",
        "\n",
        "The optional cell below closes the Postgres connection in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "0ee1bac4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ee1bac4",
        "outputId": "09d1306f-9341-48bd-bad0-6c64da031cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PostgreSQL connection closed.\n"
          ]
        }
      ],
      "source": [
        "# Optional: close the database connection when you are done\n",
        "cur.close()\n",
        "conn.close()\n",
        "print(\"PostgreSQL connection closed.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}